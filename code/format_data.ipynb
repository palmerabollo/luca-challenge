{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Format misc\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "#Plot\n",
    "import seaborn as sns\n",
    "import pylab as plt\n",
    "\n",
    "#Scrape\n",
    "import statsmodels.formula.api as smf\n",
    "import grequests\n",
    "import requests\n",
    "import bs4 as bs\n",
    "\n",
    "#Detect language\n",
    "import sys\n",
    "sys.path.append(\"./\")\n",
    "\n",
    "#Combine sets and calculate diversity\n",
    "from scipy.stats import entropy\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash2links = pickle.load(open(\"../../data_clean/hash2links.dump\",\"rb+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download main dataset (searches of people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Download data only if not previously downloaded to save time and bandwidth\n",
    "if not os.listdir(\"data\"):\n",
    "    !mkdir data\n",
    "    for month in range(7, 10):\n",
    "        month = \"{:02.0f}\".format(month)\n",
    "        for day in range(1, 32)\n",
    "            if (month==7) & (day<5):\n",
    "                continue\n",
    "            if (month==9) & (day>30):\n",
    "                continue\n",
    "            day = \"{:02.0f}\".format(day) \n",
    "            !wget -P data https://datenspende.algorithmwatch.org/btw17/download/public/datenspende_btw17_public_data_2017-\"$month\"-\"$day\".7z\n",
    "            !7z x -odata data/datenspende_btw17_public_data_2017-\"$month\"-\"$day\".7z\n",
    "            !rm data/datenspende_btw17_public_data_2017-\"$month\"-\"$day\".7z\n",
    "else:\n",
    "    print(\"Data already downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_user(df,i):\n",
    "    if int(len(df)/16) != len(df)/16:\n",
    "        print(len(df),i)\n",
    "        df[\"user\"] = np.NaN#[date+_ for _ in sorted(list(range(int(len(df)/16)))*16)]\n",
    "    else:\n",
    "        df[\"user\"] = [i+\"_\"+str(_) for _ in sorted(list(range(int(len(df)/16)))*16)]\n",
    "    return df\n",
    "    \n",
    "##Format all the json files\n",
    "files = [_ for _ in os.listdir(\"./data\") if \"json\" in _]\n",
    "hash2links = dict()\n",
    "print(len(files))\n",
    "\n",
    "    \n",
    "for i,file in enumerate(files):\n",
    "    print(i, end=\": \")\n",
    "    date = file[-15:-5] +\"_\"\n",
    "    d = json.load(open(\"./data/\"+file))\n",
    "    for result in d[-1]:\n",
    "        if hash2links.get(result[\"result_hash\"]) is None:\n",
    "            hash2links[result[\"result_hash\"]] = set([_[\"sourceUrl\"] for _ in result[\"result\"]])\n",
    "            \n",
    "    df = pd.concat([add_user(pd.DataFrame(_), date+str(j)) for j,_ in enumerate(d[:-1])])\n",
    "    df.to_csv(\"./data/people_{}.tsv\".format(file[:-5]),sep=\"\\t\",index=None)\n",
    "#     df_hash = pd.DataFrame(d[-1])\n",
    "#     df_hash.to_csv(\"./data/hash_{}.tsv\".format(file[:-5]),sep=\"\\t\",index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Save\n",
    "pickle.dump(hash2links,open(\"data_clean/hash2links.dump\",\"wb+\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Go from postal code to constituency\n",
    "const2postal = dict()\n",
    "with open(\"const2postal.csv\") as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            a,b = line.split(\":\")\n",
    "            const2postal[int(b.strip()[:-1])] = int(a.strip())\n",
    "        except:\n",
    "            pass#print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concat all csvs\n",
    "files = [_ for _ in os.listdir(\"./data\") if \"people_\" in _]\n",
    "people = pd.concat([pd.read_csv(\"data/\"+file,sep=\"\\t\") for file in files])\n",
    "\n",
    "people = people.loc[people[\"geo_location\"].str.slice(0,2)==\"DE\"]\n",
    "\n",
    "people[\"geo_location2\"] = people[\"geo_location\"].str.split(\" \").str[1]\n",
    "people = people.loc[people[\"geo_location2\"].str.len() > 1]\n",
    "people = people.loc[people[\"search_date\"]<\"2017-09-24\"]\n",
    "people[\"const_i\"] = people[\"geo_location2\"].astype(int).map(const2postal)\n",
    "print(people.loc[np.isnan(people[\"const_i\"]),\"geo_location2\"].unique())\n",
    "#['45403' '09060' '63405' '33311' '83739' '09039' '40832' '09202' '09359']\n",
    "\n",
    "people = people.loc[~np.isnan(people[\"const_i\"])]\n",
    "people.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.to_csv(\"data_clean/all_people.csv\",sep=\"\\t\",index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create ideology from Twitter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data_social_media\n",
    "#Download file ZA6926_data_v1-0-0.csv.zip from https://dbk.gesis.org/dbksearch/sdesc2.asp?no=6926\n",
    "!unzip data_social_media/ZA6926_data_v1-0-0.csv.zip data_social_media/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The following code downloads 22,123,230 tweets and takes ~1 week\n",
    "## To run it requires you to configure twarc correctly: https://medium.com/on-archivy/on-forgetting-e01a2b95272\n",
    "\n",
    "# with open(\"data_social_media/tweets.json\".format(j)) as f:\n",
    "#     for i,line in enumerate(f):\n",
    "#         pass\n",
    "# max_i = (i+1)\n",
    "\n",
    "# with open(\"data_social_media/twitter_IDs.utf-8.csv\") as f,open(\"data_social_media/remaining_twitter_IDs.utf-8.csv\",\"w+\") as f2:\n",
    "#     for i,line in enumerate(f):\n",
    "#         if i < max_i:\n",
    "#             pass\n",
    "#         else:\n",
    "#             f2.write(line)\n",
    "            \n",
    "# tweets_i += 1\n",
    "# print(tweets_i)\n",
    "# print(max_i)\n",
    "\n",
    "# !twarc hydrate data_social_media/remaining_twitter_IDs.utf-8.csv > data_social_media/tweets1.json\n",
    "# !cat data_social_media/tweets1.json >> data_social_media/tweets.json \n",
    "# !rm data_social_media/tweets1.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codebook = {1: \"CDU\",\n",
    "2: \"SPD\",\n",
    "3: \"Linke\",\n",
    "4: \"Gr√ºne\",\n",
    "5: \"CSU\",\n",
    "6: \"FDP\",\n",
    "7: \"AfD\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = pd.read_csv(\"data_social_media/candidates.utf-8.csv\",sep=\",\",usecols=[\"twlink\",\"party\"]).dropna()\n",
    "candidates[\"party\"] = candidates[\"party\"].map(codebook)\n",
    "\n",
    "def agg_tw(links):\n",
    "    return \":::\".join([_.rsplit(\"/\")[-1].lower() for _ in links])\n",
    "\n",
    "\n",
    "candidates[\"twlink\"] = candidates[\"twlink\"].str.split(\"/\").str[-1].str.lower()\n",
    "orgs = pd.read_csv(\"data_social_media/twitter_organizations.utf-8.csv\",sep=\",\").dropna()\n",
    "orgs = orgs.loc[orgs[\"group_or_party\"]!=\"media\"]\n",
    "orgs[\"screenName\"] = orgs[\"screenName\"].str.lower()\n",
    "candidates.columns = [\"name\",\"party\"]\n",
    "orgs.columns = [\"name\",\"party\"]\n",
    "cs = pd.concat([orgs,candidates])\n",
    "\n",
    "\n",
    "tw = set(cs[\"name\"])\n",
    "c2party = cs.set_index(\"name\").to_dict()[\"party\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "with open(\"./data_social_media/tweets.json\") as f, open(\"./data_social_media/twitter_media.tsv\",\"a+\") as fout, open(\"./data_social_media/twitter_media_fail.tsv\",\"a+\") as ffail:\n",
    "    for i,line in enumerate(f):\n",
    "        if i < max_i:\n",
    "            continue\n",
    "        if i%100000 == 0:\n",
    "            print(i, end=\": \")\n",
    "        try:\n",
    "            d = json.loads(line)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        if d[\"user\"][\"screen_name\"].lower() in tw:\n",
    "            id_,user = d[\"id\"],d[\"user\"][\"screen_name\"].lower()\n",
    "            try:\n",
    "                s = \"\\t\".join([str(id_),c2party[user],user,\":::\".join([requests.head(url[\"expanded_url\"], allow_redirects=True).url for url in d[\"entities\"][\"urls\"]])])\n",
    "                fout.write(s+\"\\n\")\n",
    "            except:\n",
    "                ffail.write(line)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data_social_media/twitter_media.tsv\",sep=\"\\t\",header=None,names=[\"id\",\"party\",\"tw\",\"link\"]).dropna()\n",
    "from urllib.parse import urlparse\n",
    "df = df.loc[df[\"party\"]!=\"Pegida\"]\n",
    "df[\"netloc\"] = df[\"link\"].apply(lambda x: urlparse(x).netloc.split(\".\")[-2])\n",
    "df[\"count_netloc\"] = df.groupby([\"netloc\"])[\"id\"].transform(len)\n",
    "df = df.loc[df[\"count_netloc\"]>10]\n",
    "df[\"count\"] = df.groupby([\"party\",\"netloc\"])[\"id\"].transform(len)\n",
    "df[\"count_p\"] = df.groupby([\"party\"])[\"id\"].transform(len)\n",
    "df = df.groupby([\"party\",\"netloc\",\"count\",\"count_p\"]).sum().reset_index()\n",
    "df[\"ratio\"] = df[\"count\"]/df[\"count_p\"]\n",
    "df.head()\n",
    "\n",
    "df[\"sum_netloc\"] = df.groupby(\"netloc\")[\"ratio\"].transform(np.sum)\n",
    "df[\"ideology\"] = df[\"ratio\"]/df[\"sum_netloc\"]\n",
    "del df[\"id\"]\n",
    "\n",
    "df2 = pd.pivot_table(df,values=\"ideology\",index=\"netloc\",columns=\"party\").dropna(thresh=1)\n",
    "#df2[\"div\"] = df2.apply(lambda x: entropy(np.nan_to_num(x)+0.001),1)\n",
    "#df2.sort_values(by=\"div\",ascending=False)\n",
    "df2.head()\n",
    "\n",
    "\n",
    "display(np.round(df2.loc[[\"jungefreiheit\",\"cicero\",\"welt\",\"bild\",\"faz\",\"zeit\",\"taz\",\"jungle\",\"jungewelt\"],[\"AfD\",\"CSU\",\"CDU\",\"FDP\",\"SPD\",\"Gr√ºne\",\"Linke\"]]*100,1).fillna(0))\n",
    "\n",
    "\n",
    "df2[\"sqr\"] = np.sqrt((df2>0).sum(1))\n",
    "df2[\"std\"] = df2[['AfD', 'CDU', 'CSU', 'FDP', 'Gr√ºne', 'Linke', 'SPD']].std(1)\n",
    "df2[\"std2\"] = (df2[\"std\"]*df2[\"sqr\"]).fillna(1)\n",
    "\n",
    "df2.sort_values(by=\"std2\",ascending=False).tail(100).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pylab as plt\n",
    "\n",
    "sns.clustermap(df2.fillna(0),method=\"complete\",metric=\"cosine\",figsize=(5,20),cmap=\"YlOrBr\",vmax=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "netloc2freqs= df2.fillna(0).apply(lambda x: [np.array(x)],axis=1).to_dict()\n",
    "pickle.dump(netloc2freqs,open(\"./data/netloc2freqs.dump\",\"wb+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash2links = pickle.load(open(\"data_clean/hash2links.dump\",\"rb+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = pd.read_csv(\"data_clean/all_people.csv\",sep=\"\\t\")\n",
    "people[\"day\"] = people[\"search_date\"].str.split(\" \").str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_keywords = {'Katrin%20G%C3%B6ring-Eckardt':'Katrin G√∂ring-Eckardt',\n",
    "               'Angela%20Merkel': 'Angela Merkel',\n",
    "               'Christian%20Lindner': 'Christian Lindner',\n",
    "                'Alexander%20Gauland': 'Alexander Gauland',\n",
    "               'Die%20Linke': 'Die Linke', \n",
    "                'Sahra%20Wagenknecht': 'Sahra Wagenknecht' ,\n",
    "                'Dietmar%20Bartsch': 'Dietmar Bartsch',\n",
    "               'Martin%20Schulz':'Martin Schulz', \n",
    "               'B%C3%BCndnis90/Die%20Gr%C3%BCnen':'B√ºndnis90/Die Gr√ºnen', \n",
    "                'Alice%20Weidel': 'Alice Weidel',\n",
    "               'Cem%20%C3%96zdemir': 'Cem √ñzdemir',\n",
    "                'Angelasdfsdf√∂√∂√∂√∂√∂ Merkel': 'Angela Merkel',\n",
    "               'Martin√∂√∂√∂√∂ Schulz':'Martin Schulz'}\n",
    "\n",
    "people[\"keyword\"] = people[\"keyword\"].replace(fix_keywords)\n",
    "people[\"keyword\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person2party = {\n",
    "'Katrin G√∂ring-Eckardt':'B√ºndnis90/Die Gr√ºnen',\n",
    "'Christian Lindner':'FDP',\n",
    "'Alexander Gauland': 'AfD',\n",
    "'Martin Schulz':'SPD', \n",
    "'Sahra Wagenknecht':'Die Linke',\n",
    "'Angela Merkel':'CDU',\n",
    "'Alice Weidel': 'AfD',\n",
    "'Dietmar Bartsch':'Die Linke',\n",
    "'Cem √ñzdemir':'B√ºndnis90/Die Gr√ºnen'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ideology of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "netloc2freqs = pickle.load(open(\"./data/netloc2freqs.dump\",\"rb+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add average and std happiness\n",
    "def get_ideology(x):\n",
    "    hash_,keyword = x\n",
    "    #All links hap\n",
    "#     a,lo,le,nl,e = [],[],[],[],[]\n",
    "    a = []\n",
    "    for l in hash2links[hash_]:\n",
    "        l = urlparse(l).netloc.split(\".\")\n",
    "        if len(l)<2:\n",
    "            continue\n",
    "        l = l[-2]\n",
    "#         if l in stop_links_all_even_n[keyword]:\n",
    "#             continue\n",
    "        lf = netloc2freqs.get(l)\n",
    "        if lf is None:\n",
    "            continue\n",
    "        a.append(lf)\n",
    "#         if l in stop_links_all_logged_ext_n:\n",
    "#             le.append(lf)\n",
    "#         if l in stop_links_all_notlogged_n:\n",
    "#             nl.append(lf)\n",
    "#         if l in stop_links_all_even_n:\n",
    "#             e.append(lf)\n",
    "#         if l in stop_links_all_logged_n:\n",
    "#             lo.append(lf)            \n",
    "\n",
    "#     means = []\n",
    "#     for v in [a,lo,le,nl,e]:\n",
    "#         try:\n",
    "#             means.append(np.nanmean(np.concatenate(v,0),0))\n",
    "#         except:\n",
    "#             means.append(np.array([np.NaN]*7))\n",
    "\n",
    "    if len(a) > 0:\n",
    "        return np.nanmean(np.concatennetloc2freqsate(a),0),np.std(np.concatenate(a),0),len(a) #AfD \tCDU \tCSU \tFDP \tGr√ºne \tLinke \tSPD\n",
    "    else:\n",
    "        return np.array([np.NaN]*7),np.array([np.NaN]*7),len(a)\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "with Pool() as pool:\n",
    "    people[\"av_ideo_all\"],people[\"std_ideo_all\"],people[\"count_ideo_all\"] = zip(*pool.map(get_ideology,zip(people[\"result_hash\"],people[\"keyword\"])))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# people[\"ideo_all\"],people[\"ideo_l\"],people[\"ideo_le\"],\\\n",
    "#     people[\"ideo_nl\"],people[\"ideo_ideo_le\"] = zip(*res)  \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people[\"user2\"] = people[\"user\"]+\"_\"+people[\"login_status\"].astype(int).astype(str)\n",
    "p2i = {\"AfD\":0,\"CDU\":1,\"CSU\":2,\"FDP\":3,\"Gr√ºne\":4,\"Linke\":5,\"SPD\":6}\n",
    "\n",
    "for i in p2i:\n",
    "    people[\"{}_ideo\".format(i)] = people[\"av_ideo_all\"].str[p2i[i]]\n",
    "    people[\"{}_std_ideo\".format(i)] = people[\"std_ideo_all\"].str[p2i[i]]\n",
    "    \n",
    "del people[\"av_ideo_all\"]\n",
    "del people[\"std_ideo_all\"]\n",
    "\n",
    "people.to_csv(\"data_clean/all_people_hap.csv\",sep=\"\\t\",index=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
