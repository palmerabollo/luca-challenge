{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Format misc\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "#Plot\n",
    "import seaborn as sns\n",
    "import pylab as plt\n",
    "\n",
    "#Scrape\n",
    "import statsmodels.formula.api as smf\n",
    "import grequests\n",
    "import requests\n",
    "import bs4 as bs\n",
    "\n",
    "#Detect language\n",
    "import sys\n",
    "sys.path.append(\"./\")\n",
    "\n",
    "#Combine sets and calculate diversity\n",
    "from scipy.stats import entropy\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "netloc2freqs = pickle.load(open(\"./data/netloc2freqs.dump\",\"rb+\"))\n",
    "people = pd.read_csv(\"data_clean/all_people_hap.csv\",sep=\"\\t\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def mkunion(*args):\n",
    "    args = [hash2links[_] for _ in args[0].values]\n",
    "    all_links = list(Counter(list(itertools.chain(*args))).values())\n",
    "    return entropy(all_links)/np.log2(len(all_links))\n",
    "\n",
    "p = people.groupby([\"const_i\",\"keyword\",\"login_status\",\"search_type\"]).agg({\"plugin_id\":len,\"result_hash\":mkunion}).reset_index()\n",
    "p[\"const_i\"] = p[\"const_i\"].astype(int)\n",
    "#p.to_csv(\"data_clean/groupped_all_people.csv\",sep=\"\\t\",index=None)\n",
    "p = p.loc[p[\"plugin_id\"]>0] #count of results\n",
    "p.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = p.loc[p[\"plugin_id\"]>1] #count of results\n",
    "p[\"plugin_id\"] = np.log10(p[\"plugin_id\"])\n",
    "\n",
    "parties = ['AfD',      'Bündnis90/Die Grünen', 'CDU', 'CSU',  'Die Linke',  'FDP', 'SPD']\n",
    "p[\"person\"] = p[\"keyword\"].apply(lambda x: x in parties)\n",
    "\n",
    "palette = ((255/255,174/255,143/255),(111/255,158/255,206/255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ress = []\n",
    "for a,b in p.groupby(\"search_type\"):\n",
    "    print(a)\n",
    "    for d,h in b.groupby([\"keyword\"]):\n",
    "        print(d)\n",
    "        plt.figure(figsize=(5,3.5))\n",
    "        for j,h2 in h.groupby(\"login_status\"):\n",
    "            \n",
    "            if not j:\n",
    "                color=palette[0]\n",
    "            else:\n",
    "                color=palette[1]\n",
    "            sns.regplot(\"plugin_id\",\"result_hash\",data=h2,truncate=True,\n",
    "                        scatter_kws={\"s\":10,\"alpha\":0.5},lowess=True,color=color)#,y_partial=\"plugin_id\")         \n",
    "            \n",
    "                \n",
    "            plt.scatter([0,0],[0,0],label=str(j),color=color)\n",
    "                \n",
    "            plt.xlabel(\"Log10 number of people\")\n",
    "            plt.ylabel(\"Entropy\")\n",
    "            \n",
    "            \n",
    "            ress.append((a,d,h2))\n",
    "            \n",
    "\n",
    "#             sns.regplot(\"plugin_id\",\"result_hash\",data=h2,label=str(j),truncate=True,\n",
    "#                         scatter_kws={\"s\":5,\"alpha\":0.5},y_partial=\"plugin_id\")\n",
    "#             slope, intercept = np.polyfit(h2[\"plugin_id\"], h2[\"result_hash\"],1)\n",
    "#             h2[\"new_hash\"] = h2[\"result_hash\"]+slope*h2[\"plugin_id\"]\n",
    "#             sns.regplot(\"plugin_id\",\"new_hash\",data=h2,label=str(j)+\"D\",truncate=True,\n",
    "#                         scatter_kws={\"s\":5,\"alpha\":0.5},y_partial=\"plugin_id\")            \n",
    "#             print(slope,intercept)\n",
    "#             print(smf.ols(\"result_hash~plugin_id\",data=h2).fit().summary())\n",
    "#             continue\n",
    "#         d()\n",
    "            \n",
    "        plt.xlim(1,3)\n",
    "        if a == \"search\":\n",
    "            plt.ylim(0.45,0.65)\n",
    "        else:\n",
    "            plt.ylim(0.55,0.7)\n",
    "        plt.legend()\n",
    "        \n",
    "        if d == \"Alice Weidel\":\n",
    "            sns.despine()\n",
    "            plt.grid()\n",
    "            \n",
    "            legend = plt.legend(title=\"Login status:\",fontsize=14)\n",
    "            \n",
    "            plt.setp(legend.get_title(),fontsize='small')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(\"figures/alice_{}.pdf\".format(a))\n",
    "            plt.show()\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            plt.clf()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ideology between parties\n",
    "\n",
    "person2party_code = {'Alexander Gauland': 'AfD',\n",
    " 'Alice Weidel': 'AfD',\n",
    " 'Angela Merkel': 'CDU',\n",
    " 'Cem Özdemir': 'Grüne',\n",
    " 'Christian Lindner': 'FDP',\n",
    " 'Dietmar Bartsch': 'Linke',\n",
    " 'Katrin Göring-Eckardt': 'Grüne',\n",
    " 'Martin Schulz': 'SPD',\n",
    " 'Sahra Wagenknecht': 'Linke',\n",
    "                    'Bündnis90/Die Grünen': 'Grüne',\n",
    "                    'Die Linke': \"Linke\"}\n",
    "\n",
    "\n",
    "p_i = people.groupby([\"const_i\",\"keyword\",\"login_status\",\"search_type\"]).mean().reset_index()\n",
    "\n",
    "sns.clustermap(people.loc[people[\"search_type\"]==\"news\",[\"{}_ideo\".format(party) for party in [\"AfD\",\"CDU\",\"CSU\",\"FDP\",\"Grüne\",\"Linke\",\"SPD\"]]].fillna(0).corr(),\n",
    "           cmap=\"YlOrBr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Political personalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(df, n=1, axis=0):     \n",
    "    df = df.copy().reset_index()\n",
    "    del df[\"index\"]\n",
    "    for _ in range(n):\n",
    "        df.apply(np.random.shuffle, axis=axis)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final1 = []\n",
    "final1_null = []\n",
    "for l,data in people.loc[(people[\"person\"]==0)&(people[\"count_ideo_all\"]>0)].groupby([\"login_status\",\"search_type\"]):\n",
    "    for party in [\"AfD\",\"Linke\",\"FDP\",\"Grüne\",\"CDU\",\"CSU\",\"SPD\"]:\n",
    "        vals = \"diff_{}_ideo\".format(party)\n",
    "        #vals = \"diff_std_haps\"\n",
    "        data1 = data.loc[:,[vals,\"const_i\",\"keyword\",\"user\"]]\n",
    "#         cand = party2person.get(party)\n",
    "#         if cand is None:\n",
    "#             continue\n",
    "#         data1 = data1.loc[data1[\"keyword\"].isin(cand)]\n",
    "        all_keyword = pd.pivot_table(data1,values=vals,columns=\"keyword\",index=[\"const_i\",\"user\"]).replace([0,1],np.NaN)\n",
    "        persons = list(all_keyword.columns)\n",
    "        all_keyword = all_keyword.reset_index()\n",
    "        corr = []\n",
    "        corr_null = []\n",
    "        for k in range(len(persons)):\n",
    "            for j in range(k+1,len(persons)):\n",
    "                t = all_keyword[[\"const_i\",persons[k],persons[j]]].dropna()\n",
    "                #print(len(t))\n",
    "                #if len(t) > 1000:\n",
    "                c,p = pearsonr(t[persons[k]],t[persons[j]])\n",
    "                corr.append(c)\n",
    "                \n",
    "                \n",
    "                null = pd.concat([shuffle(d) for x,d in t.groupby([\"const_i\"])])\n",
    "                c_null,p = pearsonr(null[persons[k]],null[persons[j]])\n",
    "                corr_null.append(c_null)\n",
    "        \n",
    "                final1.append(list(l)+[party]+corr)\n",
    "                final1_null.append(list(l)+[party]+corr_null)\n",
    "        #if party == \"AfD\":\n",
    "        #if l[1] == \"news\":\n",
    "        print(party,l,np.mean(corr),np.mean(corr_null))\n",
    "            #display((corr-np.identity(len(corr))).mean().reset_index().set_index(\"keyword\").transpose())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,3.5))\n",
    "\n",
    "plt.subplot(1,1,1)\n",
    "x = pd.DataFrame(final1)\n",
    "x = pd.melt(x,id_vars=[0,1,2])\n",
    "\n",
    "sns.barplot(x=1,y=\"value\",hue=0,data=x,palette=palette,edgecolor=\"none\")\n",
    "\n",
    "\n",
    "\n",
    "sns.despine()\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"Average correlation between\\n the ideology of results\")\n",
    "plt.xticks([0.,1.],[\"Google\\n News\",\"Google\\n Search\"])\n",
    "plt.ylim(-0.0,0.14)\n",
    "legend = plt.legend(fontsize=12)\n",
    "legend.set_title(\"Login status:\", prop = {'size':'small'})\n",
    "# # plt.title(\"(A) Data\")\n",
    "# plt.tight_layout()\n",
    "\n",
    "# plt.subplot(1,2,2)\n",
    "# x = pd.DataFrame(final1_null)\n",
    "# x = pd.melt(x,id_vars=[0,1,2])\n",
    "\n",
    "# sns.barplot(x=1,y=\"value\",hue=0,data=x,palette=palette,edgecolor=\"none\")\n",
    "# legend = plt.legend(title=\"Logged in:\",fontsize=14)\n",
    "\n",
    "\n",
    "# sns.despine()\n",
    "# plt.xlabel(\"\")\n",
    "# plt.ylabel(\"Average correlation between\\n ideology of links\")\n",
    "\n",
    "# plt.ylim(-0.0,0.14)\n",
    "# plt.title(\"(B) Null model\")\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.savefig(\"figures/corr_ideo.pdf\".format(a))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.DataFrame(final1)\n",
    "x = pd.melt(x,id_vars=[0,1,2,3])\n",
    "plt.figure(figsize=(10,4))\n",
    "sns.barplot(x=2,y=\"value\",hue=0,data=x.loc[x[1]==\"news\"])\n",
    "plt.figure(figsize=(10,4))\n",
    "sns.barplot(x=2,y=\"value\",hue=0,data=x.loc[x[1]==\"search\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "media = np.array([0.8,0.2])\n",
    "own_op = 1\n",
    "p = 0.\n",
    "## Model\n",
    "\n",
    "def update(x,w=None):\n",
    "    #return (own_op*x+x**b*np.mean(media))/(own_op+x**b*np.mean(media) + (1-x)**b*(1-np.mean(media)))\n",
    "    \n",
    "#     diffs = np.abs(media-x)\n",
    "#     f = np.abs(np.diff(diffs))[0]*slope\n",
    "#     w = np.array([0.5,0.5])\n",
    "#     w[diffs==np.min(diffs)] += f\n",
    "#     w[diffs==np.max(diffs)] -= f\n",
    "    if w is None:\n",
    "        w = 1-np.abs(x-media)\n",
    "        w = w/np.sum(w)\n",
    "#    w = 0.5\n",
    "    \n",
    "    #Linear increase\n",
    "    return (own_op*x+x**(b+p)*np.sum(media*w))/(own_op+x**(b+p)*np.sum(media*w) + (1-x)**(b+p)*(1-np.sum(media*w)))\n",
    " \n",
    "plt.figure(figsize=(10,3.5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "for b in [0.6,0.7,0.8,0.9,1,1.1]:# np.linspace(0,1.1,12):\n",
    "    x = [0.01,0.1,0.49,0.51,0.9,0.99]\n",
    "    v = []\n",
    "    for i in x:\n",
    "        for j in range(1000):\n",
    "            i = update(i,w=0.5)\n",
    "        v.append(i)\n",
    "    plt.grid()\n",
    "    plt.plot(x,v,label=str(b)[:4],color=(1.4-b,1.4-b,1.4-b),linewidth=2)\n",
    "    if b == 1:\n",
    "        plt.annotate(r\"$b=1$\",(0.9,0.8))\n",
    "    if b == 1.1:\n",
    "        plt.annotate(r\"$b>1$\",(0.9,1.05)) \n",
    "    if b == 0.6:\n",
    "        plt.annotate(r\"$b<1$\",(0.9,0.4))\n",
    "    #print(v[-2])\n",
    "# plt.legend(title=\"Assimilation bias\")\n",
    "sns.despine()\n",
    "plt.xlabel(\"Initial opinion\")\n",
    "plt.ylabel(\"Final opinion\")\n",
    "plt.title(\"(A): No personalization\")\n",
    "plt.ylim(0,1.1)\n",
    "plt.xlim(0,1.05)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "for b in [0.6,0.7,0.8,0.9,1,1.1]:# np.linspace(0,1.1,12):\n",
    "    x = [0.01,0.1,0.49,0.51,0.9,0.99]\n",
    "    v = []\n",
    "    for i in x:\n",
    "        for j in range(1000):\n",
    "            i = update(i)\n",
    "        v.append(i)\n",
    "    plt.grid()\n",
    "    plt.plot(x,v,label=str(b)[:3],color=(1.4-b,1.4-b,1.4-b),linewidth=2)\n",
    "\n",
    "    #print(v[-2])\n",
    "# plt.legend(loc=4)\n",
    "# plt.plot([0.82,0.82],[0.08,0.67],color=\"gray\")\n",
    "# plt.annotate(r\"$b$\",(0.78,0.4))\n",
    "\n",
    "plt.annotate(r\"$b=0.6$\",(1.,0.65))\n",
    "plt.annotate(r\"$b=0.7$\",(1,0.8))\n",
    "plt.annotate(r\"$b=0.8$\",(1,0.93))\n",
    "plt.annotate(r\"$b>0.8$\",(1,1.05))\n",
    "\n",
    "sns.despine()\n",
    "plt.xlabel(\"Initial opinion\")\n",
    "plt.ylabel(\"Final opinion\")\n",
    "plt.title(\"(B): Personalization\")\n",
    "plt.ylim(0,1.1)\n",
    "plt.xlim(0,1.15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/sim_polarization.pdf\")\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
